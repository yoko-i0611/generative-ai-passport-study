# AI講師のモデルアップグレードについて

## 現在の設定

### モデル
- **GPT-4o** を使用（`process.env.OPENAI_MODEL || 'gpt-4o'`）
- 以前は GPT-3.5-turbo を使用していましたが、より深みのある回答のためにアップグレードしました

### パラメータ
- **maxTokens**: 1500（以前: 1000）
- **temperature**: 0.7（以前: 0.8）- 創造性と正確性のバランスを取る

### プロンプト改善
- 「なるほど！」と思える深みのある解説を促すプロンプトに改善
- 背景や理由の説明を重視
- 技術の比較や対比、実用例の提供を指示

## 回答速度について

### ストリーミング表示速度
- **streamDelayMs**: 15ms（以前: 30ms）
- 実際のAPI応答時間はモデルとネットワークに依存します

### 速度向上のための選択肢

1. **GPT-4o-mini** の使用（コスト削減、速度向上）
   - `OPENAI_MODEL=gpt-4o-mini` を環境変数に設定
   - 品質は若干下がる可能性がありますが、速度は向上します

2. **ストリーミングAPIの実装**（推奨）
   - 現在は模擬ストリーミング（文字を1文字ずつ表示）
   - 実際のストリーミングAPIを使用すると、最初のトークンが到着次第表示を開始できます
   - これは最大の速度向上を実現します

3. **キャッシュの活用**
   - 同じ質問への回答をキャッシュすることで、2回目以降の表示を高速化

## 環境変数の設定

`.env.local` ファイルに以下を追加することで、使用するモデルを変更できます：

```bash
OPENAI_MODEL=gpt-4o          # デフォルト（高品質、中速）
# OPENAI_MODEL=gpt-4o-mini   # 高速、低コスト（品質は若干下がる可能性）
# OPENAI_MODEL=gpt-3.5-turbo # 最速、低コスト（品質は下がる）
```

## 今後の改善案

1. **実際のストリーミングAPIの実装**
   - `/api/chat-stream` エンドポイントの作成
   - AI SDKの `useChat` フックの使用

2. **回答品質のモニタリング**
   - ユーザーフィードバックを収集
   - プロンプトの継続的な改善


